{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import operator\n",
    "from collections import Counter\n",
    "from bitarray import bitarray\n",
    "import mmh3\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Task\n",
    "\n",
    "### Download\tthe\tHoneypot\tdata (34\tGB,\tbut\twe\tonly\tuse\tthe\tfirst\tIP\taddress\tvalues).\tLoad\tone\tof\tthe\tfiles\tin\tyour\tfavorite\teditor\tand\tdescribe\tin\ta\tfew\tlines\twhat\tyou\tsee.\n",
    "\n",
    "What we see when we open the file in Sublime Text 2 is the following. On the first glimpse it looks quite chaotic.\n",
    "\n",
    "\n",
    "![title](data/dataset.png)\n",
    "\n",
    "However, if we looked closer we can indeed see that it is a honeypot shell file. We can observe that the data is timestamped, it includes shell scripts and the protocol with which the files were transfered between the different IPs. The study of files such as this one could potentialy help on identifying certain types of malicious behavior, what the goals of the attackers are or what are the methods they use to attack. Particularly, the information we have is: time, source address, src port, dst address, dst port, and a list of issued commands.\n",
    "\n",
    "\n",
    "The part of the dataset that we are interested in is the IPs after the date. Thus, we extracted those IPs and we created another file \"src_ip.txt\" which only contains those IPs in temporal order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now count\tthe number of distinct source IP_addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct IP addresses is: 523443\n"
     ]
    }
   ],
   "source": [
    "# Reading the file which contains the source IP addresses (given)-\n",
    "src_ip = open(\"data/src_ip.txt\", 'r')\n",
    "\n",
    "# Transforming the file to a list\n",
    "stream_ips = [line[:-2] for line in src_ip.readlines()]\n",
    "\n",
    "# Printing the number of distinct source IP addresses\n",
    "print(\"The number of distinct IP addresses is: %s\" %len(set(stream_ips)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the 10 most frequent IP addresses?\n",
    "\n",
    "To answer this question we used the module Counter from the package collections. Below, we calculate the frequency of each IP address, then we sort them in descending order and we print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('115.177.11.21', 294690)\n",
      "('115.176.182.19', 259241)\n",
      "('192.3.106.4', 66572)\n",
      "('191.96.249.18', 51251)\n",
      "('104.192.0.2', 26348)\n",
      "('198.204.234.2', 22229)\n",
      "('185.93.185.1', 5157)\n",
      "('198.204.234.3', 5049)\n",
      "('115.29.251.11', 3812)\n",
      "('104.192.0.1', 3701)\n"
     ]
    }
   ],
   "source": [
    "# count the frequency of each IP address\n",
    "freq = Counter(stream_ips)\n",
    "\n",
    "# Sort them according to their value to find the 13 most frequent ones\n",
    "sorted_el = sorted(freq.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "# Print the 10 most frequent IP addresses\n",
    "for i in range(10):\n",
    "    print (sorted_el[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code\tfor\tthe\tFREQUENT\talgorithm,\tuse\tit\tto\tcount\tthe\tamounts\tin\tone\tpass\n",
    "\n",
    "The implementation of the FREQUENT (Misra - Gries) algorithm is presented below.\n",
    "\n",
    "### What\tare\tthe\t10\tmost\tfrequent\tIP-addresses\tand\ttheir\tfrequencies?\n",
    "To answer this question by using the FREQUENT algorithm, we ran the FREQUENT algorithm with a value of k equal to a very large number. The reason that we have to pick a large number for k is that since we want to count the frequency of the elements, and not the elements whose frequency exceeds the 1/k fraction of the total count.\n",
    "\n",
    "We sorted the results of the FREQUENT algorithm. The 10 most frequent IPs of our dataset are presented below in descending order.\n",
    "\n",
    "Notice that the second number in the resulted tuples is not the actual frequency of each IP (which was found before) but the counted value from the FREQUENT algorithm.\n",
    "\n",
    "We did that in order to verify that our algorithm works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementation of FREQUENT (Misra - Gries) algorithm\n",
    "def frequent(stream, k):\n",
    "    freqD = {}\n",
    "    for value in stream:\n",
    "        if value in freqD:\n",
    "            freqD[value]+=1\n",
    "\n",
    "        elif len(freqD) < k:\n",
    "            freqD[value] = 1\n",
    "\n",
    "        else:\n",
    "            for existing_value in freqD:\n",
    "                freqD[existing_value]-=1\n",
    "            freqD = {x: y for x, y in freqD.items() if y != 0}\n",
    "        #print(freqD) #(show step-by-step)\n",
    "    return freqD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most frequent IP addresses are:\n",
      "('115.177.11.21', 294690)\n",
      "('115.176.182.19', 259241)\n",
      "('192.3.106.4', 66572)\n",
      "('191.96.249.18', 51251)\n",
      "('104.192.0.2', 26348)\n",
      "('198.204.234.2', 22229)\n",
      "('185.93.185.1', 5157)\n",
      "('198.204.234.3', 5049)\n",
      "('115.29.251.11', 3812)\n",
      "('104.192.0.1', 3701)\n"
     ]
    }
   ],
   "source": [
    "# Running the FREQUENT algorithm\n",
    "\n",
    "# initialize fraction\n",
    "k = 1000000\n",
    "\n",
    "# Find elements whose frequency exceeds 1/k fraction of the total count\n",
    "elements = frequent(stream_ips, k)\n",
    "\n",
    "# Sort them according to their value to find the 13 most frequent ones\n",
    "sorted_freq = sorted(elements.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "print(\"The 10 most frequent IP addresses are:\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(sorted_freq[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As could be observed from the above results by using a very large value for k we get the exact same results as when we calculated the frequencies in the normal way.\n",
    "\n",
    "This is reasonable, as the condition of the FREQUENT algorithm \"elif len(freqD) < k-1:\" is always true. Thus, our algorithm basically just keeps a dictionary with the IPs  as keys, and increases their values as it processes the IP stream.\n",
    "\n",
    "### Run\tFREQUENT\tusing\treservoirs\tof\tsize\t10,\t100,\tand\t1000.\tWhat\tare\tthe\t10\tmost\tfrequent\tIP-addresses\tand\ttheir\tfrequencies?\n",
    "\n",
    "The code below runs the FREQUENT algorithm with k equal to 10, 100 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The 8 most frequent IP addresses with k = 10 are: \n",
      " (On the right the real 10 most frequent IP addresses are presented)\n",
      "\n",
      "FREQUENT #0: ('115.176.182.19', 79417)\t \t REAL #0: ('115.177.11.21', 294690)\n",
      "FREQUENT #1: ('191.96.249.18', 696)\t \t REAL #1: ('115.176.182.19', 259241)\n",
      "FREQUENT #2: ('187.199.79.1', 2)\t \t REAL #2: ('192.3.106.4', 66572)\n",
      "FREQUENT #3: ('114.32.130.1', 1)\t \t REAL #3: ('191.96.249.18', 51251)\n",
      "FREQUENT #4: ('115.108.40.1', 1)\t \t REAL #4: ('104.192.0.2', 26348)\n",
      "FREQUENT #5: ('87.96.167.23', 1)\t \t REAL #5: ('198.204.234.2', 22229)\n",
      "FREQUENT #6: ('187.23.203.25', 1)\t \t REAL #6: ('185.93.185.1', 5157)\n",
      "FREQUENT #7: ('82.178.61.9', 1)\t \t REAL #7: ('198.204.234.3', 5049)\n",
      "\n",
      "\n",
      "The 10 most frequent IP addresses with k = 100 are: \n",
      " (On the right the real 10 most frequent IP addresses are presented)\n",
      "\n",
      "FREQUENT #0: ('115.177.11.21', 266748)\t \t REAL #0: ('115.177.11.21', 294690)\n",
      "FREQUENT #1: ('115.176.182.19', 242626)\t \t REAL #1: ('115.176.182.19', 259241)\n",
      "FREQUENT #2: ('192.3.106.4', 43997)\t \t REAL #2: ('192.3.106.4', 66572)\n",
      "FREQUENT #3: ('191.96.249.18', 37761)\t \t REAL #3: ('191.96.249.18', 51251)\n",
      "FREQUENT #4: ('185.93.185.1', 701)\t \t REAL #4: ('104.192.0.2', 26348)\n",
      "FREQUENT #5: ('190.66.183.4', 2)\t \t REAL #5: ('198.204.234.2', 22229)\n",
      "FREQUENT #6: ('2.27.193.10', 2)\t \t REAL #6: ('185.93.185.1', 5157)\n",
      "FREQUENT #7: ('187.199.79.1', 2)\t \t REAL #7: ('198.204.234.3', 5049)\n",
      "FREQUENT #8: ('14.182.80.23', 1)\t \t REAL #8: ('115.29.251.11', 3812)\n",
      "FREQUENT #9: ('121.143.72.5', 1)\t \t REAL #9: ('104.192.0.1', 3701)\n",
      "\n",
      "\n",
      "The 10 most frequent IP addresses with k = 1000 are: \n",
      " (On the right the real 10 most frequent IP addresses are presented)\n",
      "\n",
      "FREQUENT #0: ('115.177.11.21', 292000)\t \t REAL #0: ('115.177.11.21', 294690)\n",
      "FREQUENT #1: ('115.176.182.19', 257625)\t \t REAL #1: ('115.176.182.19', 259241)\n",
      "FREQUENT #2: ('192.3.106.4', 64132)\t \t REAL #2: ('192.3.106.4', 66572)\n",
      "FREQUENT #3: ('191.96.249.18', 49635)\t \t REAL #3: ('191.96.249.18', 51251)\n",
      "FREQUENT #4: ('104.192.0.2', 23658)\t \t REAL #4: ('104.192.0.2', 26348)\n",
      "FREQUENT #5: ('198.204.234.2', 19643)\t \t REAL #5: ('198.204.234.2', 22229)\n",
      "FREQUENT #6: ('185.93.185.1', 4431)\t \t REAL #6: ('185.93.185.1', 5157)\n",
      "FREQUENT #7: ('198.204.234.3', 2657)\t \t REAL #7: ('198.204.234.3', 5049)\n",
      "FREQUENT #8: ('115.29.251.11', 1446)\t \t REAL #8: ('115.29.251.11', 3812)\n",
      "FREQUENT #9: ('104.192.0.1', 1308)\t \t REAL #9: ('104.192.0.1', 3701)\n"
     ]
    }
   ],
   "source": [
    "# Running the FREQUENT algorithm\n",
    "\n",
    "# initialize fraction\n",
    "for k in [10, 100, 1000]:\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    # Find elements whose frequency exceeds 1/k fraction of the total count\n",
    "    elements = frequent(stream_ips, k)\n",
    "\n",
    "    # Sort them according to their value to find the 13 most frequent ones\n",
    "    sorted_freq = sorted(elements.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "        \n",
    "    if len(sorted_freq)<10:\n",
    "        num = len(sorted_freq)\n",
    "    else:\n",
    "        num = 10\n",
    "        \n",
    "    print(\"The %s most frequent IP addresses with k = %s are: \\n (On the right the real 10 most frequent IP addresses are presented)\\n\" %(num,k))\n",
    "\n",
    "    for i in range(num):\n",
    "        print(\"FREQUENT #%s: %s\\t \\t REAL #%s: %s\" %(i, sorted_freq[i], i, sorted_el[i]))\n",
    "        \n",
    "        # sorted_el contains the real frequencies of the IPs.\n",
    "        # print it  as well, if you want to see the differences in the same page.\n",
    "        #print(sorted_el[i])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the results are not the same as before. \n",
    "\n",
    "#### k = 10\n",
    "\n",
    "With k = 10 the algorithm does not even return 10 values as it is looking for the IPs whose frequency exceeds the fraction of 10% over the total count of IPs. Apparently, only 7 IPs fulfill this requirement. \n",
    "\n",
    "In addition, only two of those IPs actually exist in the true 7 most frequent IPs. Those results are reasonable, as there are various IPs which have a frequency 1 according to the FREQUENT algorithm, and the sorting of them by value does not end up with the correct order. Furthermore, depending on the initial stream it is possible for a high frequently value to be decreased a lot of times (e.g if it has been increased a lot initially, and then the dictionary is full and then different IPs constantly appear) and eventually obtain a smaller frequency from others even if in reality this is not correct.\n",
    "\n",
    "#### k = 100\n",
    "\n",
    "The results in this case are closer to the ground truth. This is logical as the size of the reservoir is larger and the decreasing happens less often than before.\n",
    "\n",
    "#### k =  1000\n",
    "\n",
    "Finally, in that case the order is the same as the initial. Still, the frequencies are not the same (as they were for example with the very large k) but the ranking is.\n",
    "\n",
    "In general, the larger the size of the reservoirs the less times the decreasing occurs. Without the decreasing part, the algorithm's behaviour is the same as the simple counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing Task\n",
    "Splitting\tthe\thoneypot\tdata\t50:50\tinto\tsplit\t(first\thalf) and\ttest\t(second\thalf)\tmaintaining\tthe\ttemporal\torder.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the initial set is: 3414011\n",
      "The size of the training set is: 1707005\n",
      "The size of the testing set is: 1707006\n"
     ]
    }
   ],
   "source": [
    "# stream_ips is a list containing the IPs in temporal order\n",
    "\n",
    "train = stream_ips[:int(len(stream_ips)/2)]\n",
    "test = stream_ips[int(len(stream_ips)/2):]\n",
    "\n",
    "# Check size of train and test set\n",
    "print(\"The size of the initial set is: %s\" %len(stream_ips))\n",
    "print(\"The size of the training set is: %s\" %len(train))\n",
    "print(\"The size of the testing set is: %s\" %len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\tfor\teach\tsource\tIP in\tthe\ttest\tset\twhether\tit\toccurs\tin\tthe\ttrain\tset.\n",
    "\n",
    "We store the real number of source IPs in the test set which also occur in the train set in the variable named gt. We will use this variable later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58061 source IPs in the test set which also occur in the train set\n"
     ]
    }
   ],
   "source": [
    "# We firstly create two sets from our training and testing lists\n",
    "train_set = set(train)\n",
    "test_set = set(test)\n",
    "\n",
    "# Then we check the intersection of the two sets\n",
    "inter = set.intersection(train_set, test_set)\n",
    "\n",
    "# The real number of source IPs in the test set which also occur in the train set\n",
    "gt = len(inter) # our ground truth\n",
    "print(\"There are %s source IPs in the test set which also occur in the train set\"%gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write\tcode\tfor\ta\tBLOOM\tfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BloomFilter:\n",
    "    \n",
    "    def __init__(self, size, hash_count):\n",
    "        self.size = size\n",
    "        self.hash_count = hash_count\n",
    "        self.bit_array = bitarray(size)\n",
    "        self.bit_array.setall(0)\n",
    "        \n",
    "    def add(self, string):\n",
    "        for seed in range(self.hash_count):\n",
    "            result = mmh3.hash(string, seed) % self.size\n",
    "            self.bit_array[result] = 1\n",
    "        \n",
    "    def lookup(self, string):\n",
    "        for seed in range(self.hash_count):\n",
    "            result = mmh3.hash(string, seed) % self.size\n",
    "            if self.bit_array[result] == 0:\n",
    "                return \"Nope\"\n",
    "        return \"Probably\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what we can do with the bloom filter.\n",
    "\n",
    "\n",
    "By using the following formulas we are able to compute the size of the bloom filter and number of the hash functions to be used, assuming that we now the expected number of items and the desired false positive probability.\n",
    "\n",
    "![title](data/k.png)\n",
    "\n",
    "![title](data/m.png) \n",
    "\n",
    "We realized, that if we use the expected number of items equal to the length of the train set and a probability of 0.000001 we get the correct results (58061). Run the code below to check the bit array size and the number of hash functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bit array size: 9776213\n",
      "Number of hash functions: 20\n",
      "IPs from the training set added\n",
      "There are no more than 58061 source IPs in the test set which also occur in the train set\n"
     ]
    }
   ],
   "source": [
    "n = len(train_set) # expected number of items\n",
    "p = 0.000001 # probability percentage\n",
    "m = - (n* math.log(p)) /(math.log(2)**2) # size of the bit array\n",
    "k = (m/n)* math.log(2) # number of hash functions\n",
    "\n",
    "m = round(m)\n",
    "k = round(k)\n",
    "\n",
    "print(\"Bit array size: %s\" %m)\n",
    "print(\"Number of hash functions: %s\" %k)\n",
    "\n",
    "bf = BloomFilter(m, k)\n",
    "\n",
    "# Adding training set's IPs\n",
    "for ip in train_set:\n",
    "    bf.add(ip)\n",
    "\n",
    "print(\"IPs from the training set added\")\n",
    "\n",
    "# lookup for the IPs in the test set\n",
    "count = 0\n",
    "for ip in test_set:\n",
    "    if bf.lookup(ip) == \"Probably\":\n",
    "        count+=1\n",
    "print(\"There are no more than %s source IPs in the test set which also occur in the train set\"%count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform\tthe\tsame\ttest\tusing\tBLOOM\tfilters\tof\tsizes\t10,\t100,\tand\t1000.\tExplain\tthe\t differences\tusing\tthe\ttheoretical\tfalse\tpositive\trates.\n",
    "\n",
    "For that purpose we will use different sizes of the bit array and then derive the number of k.\n",
    "\n",
    "Again, n equals to the size of the training set.\n",
    "\n",
    "Running the code below, we found a problem. The number of hash functions, defined by the formulas, is always 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for bit array size: 10\n",
      "Number of hash functions: 0 \n",
      "\n",
      "Running for bit array size: 100\n",
      "Number of hash functions: 0 \n",
      "\n",
      "Running for bit array size: 1000\n",
      "Number of hash functions: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = len(train_set) # expected number of items\n",
    "\n",
    "results = {} # dictionary with the results for different bit array sizes\n",
    "\n",
    "for m in [10, 100, 1000]:\n",
    "    k = (m/n)* math.log(2) # number of hash functions\n",
    "    k = round(k)\n",
    "    print(\"Running for bit array size: %s\" %m)\n",
    "    print(\"Number of hash functions: %s \\n\" %k)\n",
    "    bf = BloomFilter(m, k)\n",
    " \n",
    "    # Adding training set's IPs\n",
    "    for ip in train_set:\n",
    "        bf.add(ip)\n",
    "\n",
    "    # lookup for the IPs in the test set\n",
    "    \n",
    "    for ip in test_set:\n",
    "        if bf.lookup(ip) == \"Probably\":\n",
    "            if m in results:\n",
    "                results[m].append(ip)\n",
    "            else:\n",
    "                results[m] = [ip]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it could be observed from the above results, for such a small size of the bit array the number of the hash functions which is needed is 0. Thus, to get some results we should increase the size significantly.\n",
    "\n",
    "After experimentation we found out that to get at least one hash function we should use a size of around 250.000. Thus, we decided to try the values 500.000, 1000.000, 2.000.000, 4.000.000, 8.000.000, 10.000.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for bit array size: 500000\n",
      "Number of hash functions: 1\n",
      "The number of test set's IPs which are possibly in the training set are: 148178\n",
      "The false positive rate is: 0.4912025378552507 \n",
      "\n",
      "Running for bit array size: 1000000\n",
      "Number of hash functions: 2\n",
      "The number of test set's IPs which are possibly in the training set are: 102500\n",
      "The false positive rate is: 0.24222454786277267 \n",
      "\n",
      "Running for bit array size: 2000000\n",
      "Number of hash functions: 4\n",
      "The number of test set's IPs which are possibly in the training set are: 68862\n",
      "The false positive rate is: 0.05887322715330695 \n",
      "\n",
      "Running for bit array size: 4000000\n",
      "Number of hash functions: 8\n",
      "The number of test set's IPs which are possibly in the training set are: 58671\n",
      "The false positive rate is: 0.0033249392244715527 \n",
      "\n",
      "Running for bit array size: 8000000\n",
      "Number of hash functions: 16\n",
      "The number of test set's IPs which are possibly in the training set are: 58066\n",
      "The false positive rate is: 2.72536002005865e-05 \n",
      "\n",
      "Running for bit array size: 10000000\n",
      "Number of hash functions: 20\n",
      "The number of test set's IPs which are possibly in the training set are: 58061\n",
      "The false positive rate is: 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = len(train_set) # expected number of items\n",
    "\n",
    "results = {} # dictionary with the results for different bit array sizes\n",
    "fpr = {}  # dictionary with the false positive rates for different bit array sizes\n",
    "\n",
    "# for each bit array size\n",
    "for m in [500000, 1000000, 2000000, 4000000, 8000000, 10000000]:\n",
    "    # calculate k\n",
    "    k = (m/n)* math.log(2) # number of hash functions\n",
    "    k = round(k)\n",
    "    print(\"Running for bit array size: %s\" %m)\n",
    "    print(\"Number of hash functions: %s\" %k)\n",
    "    \n",
    "    # define bloom filter\n",
    "    bf = BloomFilter(m, k)\n",
    " \n",
    "    # Adding training set's IPs\n",
    "    for ip in train_set:\n",
    "        bf.add(ip)\n",
    "\n",
    "    # lookup for the IPs in the test set\n",
    "    # and if they exist in the training set store them in results\n",
    "    for ip in test_set:\n",
    "        if bf.lookup(ip) == \"Probably\":\n",
    "            if m in results:\n",
    "                results[m].append(ip)\n",
    "            else:\n",
    "                results[m] = [ip]\n",
    "    \n",
    "    # False positive rate\n",
    "    # The total number of testing set's IPs we said they possibly exist in the training set minus \n",
    "    # the actual number of the co-occured IPs. With the bloom filter always we get more or equal number of IPs \n",
    "    # than the real number. This is happening due to the fact that if  an IP exists in both sets, bloom filter will\n",
    "    # definetly find it. The case where bloom filter might be wrong is that it could \"say\" that an IP exists \n",
    "    # in the training set while in reality it does not. It could never \"say\" that an IP that does not exist in the\n",
    "    #training set, exists. (Basically, there are no FN, only FP)\n",
    "    \n",
    "    # How many IPs of the test set are not in the training set but we said they are?\n",
    "    # Number of bloom filter results minus the correct number (ground truth)\n",
    "    fp = len(results[m]) - gt\n",
    "    tn = len(test_set) - len(results[m])\n",
    "    fpr[m] = fp/(fp+tn)\n",
    "    print(\"The number of test set's IPs which are possibly in the training set are: %s\" %len(results[m]))\n",
    "    print(\"The false positive rate is: %s \\n\" %fpr[m])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain\tthe\tdifferences\tusing\tthe\ttheoretical\tfalse\tpositive\trates.\n",
    "\n",
    "The theoretical false positive rates are given by the following formula:\n",
    "\n",
    "![title](data/fp_def.png) \n",
    "\n",
    "To count them and compare them with our results we wrote the following script which takes into account the above bit array sizes and the corresponding number of hash functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for bit array size: 500000\n",
      "Number of hash functions: 1\n",
      "The theoretical false positive rate is: 0.4933641003074374 \n",
      "Difference between the theoretical and real false positive rate is: 0.0021615624521866827 \n",
      "\n",
      "Running for bit array size: 1000000\n",
      "Number of hash functions: 2\n",
      "The theoretical false positive rate is: 0.24340796553013969 \n",
      "Difference between the theoretical and real false positive rate is: 0.001183417667367015 \n",
      "\n",
      "Running for bit array size: 2000000\n",
      "Number of hash functions: 4\n",
      "The theoretical false positive rate is: 0.05924739629536252 \n",
      "Difference between the theoretical and real false positive rate is: 0.0003741691420555693 \n",
      "\n",
      "Running for bit array size: 4000000\n",
      "Number of hash functions: 8\n",
      "The theoretical false positive rate is: 0.003510251521083801 \n",
      "Difference between the theoretical and real false positive rate is: 0.0001853122966122482 \n",
      "\n",
      "Running for bit array size: 8000000\n",
      "Number of hash functions: 16\n",
      "The theoretical false positive rate is: 1.232185713747658e-05 \n",
      "Difference between the theoretical and real false positive rate is: 1.4931743063109918e-05 \n",
      "\n",
      "Running for bit array size: 10000000\n",
      "Number of hash functions: 20\n",
      "The theoretical false positive rate is: 7.300374368283183e-07 \n",
      "Difference between the theoretical and real false positive rate is: 7.300374368283183e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tup in [(500000,1), (1000000,2), (2000000,4), (4000000,8), (8000000,16), (10000000,20)]:\n",
    "    m = tup[0]\n",
    "    k = tup[1]\n",
    "    fp = (1 - ( 1 - (1.0/m))**(k*n))**k\n",
    "    \n",
    "    print(\"Running for bit array size: %s\" %m)\n",
    "    print(\"Number of hash functions: %s\" %k)\n",
    "    print(\"The theoretical false positive rate is: %s \" %fp)\n",
    "    print(\"Difference between the theoretical and real false positive rate is: %s \\n\" %(abs(fp-fpr[m])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the above results, the differences between the theoretical and actual false positive rates values are very small. Particularly, less than 0.0022 for all the cases.\n",
    "\n",
    "The difference between the theoretical and real values was expected. The formula for the theoretical computation of the false positive rate is based on the assumption that the probabilities of each bit being set is independent (https://en.wikipedia.org/wiki/Bloom_filter). However, it is a close approximation of the real values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extend\tthe\tBLOOM\tfilter\tto\ta\tCount-Min\tsketch,\tplay\twith\tdifferent\theights\tand\twidths\tfor\tthe\tCM\tsketch\tmatrix.\n",
    "\n",
    "In the same logic with the bloom filter class we created the count-min class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CountMin:\n",
    "    \n",
    "    def __init__(self, w, d):\n",
    "        self.size = w*d\n",
    "        self.w = w\n",
    "        self.hash_count = d\n",
    "        self.cm_array =  [[0]*w for i in range(d)]\n",
    "        \n",
    "    def add(self, string):\n",
    "        for seed in range(self.hash_count):\n",
    "            result = mmh3.hash(string, seed) % self.w\n",
    "            self.cm_array[seed][result] += 1\n",
    "        \n",
    "    def point(self, string):\n",
    "        min = 1000000000000\n",
    "        for seed in range(self.hash_count):\n",
    "            result = mmh3.hash(string, seed) % self.w\n",
    "            if self.cm_array[seed][result]<min:\n",
    "                min = self.cm_array[seed][result]\n",
    "        return min\n",
    "    #def lookup(self, string):\n",
    "    #    for seed in range(self.hash_count):\n",
    "    #        result = mmh3.hash(string, seed) % self.size\n",
    "    #        if self.bit_array[result] == 0:\n",
    "    #            return \"Nope\"\n",
    "    #    return \"Probably\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play\twith\tdifferent\theights\tand\twidths\tfor\tthe\tCM\tsketch\tmatrix\n",
    "\n",
    "The count-min sketch's parameters w and d can be chosen by setting w = ⌈e/ε⌉ and d = ⌈ln 1/δ⌉, where the error in answering a query is within an additive factor of ε with probability 1 − δ , and e is Euler's number [https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch]. Thus, we decided to run the algorithm with different values of epsilon and delta which lead to different values of height and width. \n",
    "\n",
    "With the following script after creating the count-min data structure for the different epsilon and delta, we add the IPs to the structure and then we find the frequency of each IP and we store it in a dictionary (cm_res).\n",
    "\n",
    "Finally, we calculate and present the summation of the absolute difference between the count-min frequency of the 10 most frequent IPs and their actual (ground truth) frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ε = 0.0001 | δ = 0.0001 | w = 27183 | d = 9 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "383 \n",
      "\n",
      "ε = 0.0001 | δ = 0.001 | w = 27183 | d = 7 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "407 \n",
      "\n",
      "ε = 0.0001 | δ = 0.01 | w = 27183 | d = 5 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "445 \n",
      "\n",
      "ε = 0.001 | δ = 0.0001 | w = 2718 | d = 9 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "7089 \n",
      "\n",
      "ε = 0.001 | δ = 0.001 | w = 2718 | d = 7 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "7387 \n",
      "\n",
      "ε = 0.001 | δ = 0.01 | w = 2718 | d = 5 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "7872 \n",
      "\n",
      "ε = 0.01 | δ = 0.0001 | w = 272 | d = 9 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "89287 \n",
      "\n",
      "ε = 0.01 | δ = 0.001 | w = 272 | d = 7 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "90273 \n",
      "\n",
      "ε = 0.01 | δ = 0.01 | w = 272 | d = 5 \n",
      "The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \n",
      "93525 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "e = 2.718281828\n",
    "\n",
    "#initialize ε and δ\n",
    "for epsilon in [0.0001, 0.001, 0.01]:\n",
    "    for delta in [0.0001, 0.001, 0.01]:\n",
    "        # derive w and d\n",
    "        w = round(e/epsilon)\n",
    "        d = round(math.log(1/delta))\n",
    "\n",
    "        # create count min\n",
    "        cm = CountMin(w, d)\n",
    "\n",
    "        # streaming and adding to matrix\n",
    "        for ip in stream_ips:\n",
    "                cm.add(ip)\n",
    "                \n",
    "        # find frequency and store it to cm_res\n",
    "        cm_res = {}\n",
    "        for ip in stream_ips:\n",
    "            cm_res[ip] = cm.point(ip)\n",
    "\n",
    "        # Sort them according to their value to find the 10 most frequent ones\n",
    "        sorted_cm = sorted(cm_res.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "        diff = 0\n",
    "        for i in range(10):\n",
    "            diff+= abs(sorted_cm[i][1] - sorted_el[i][1])\n",
    "\n",
    "        print(\"ε = %s | δ = %s | w = %s | d = %s \" %(epsilon, delta, w, d))\n",
    "        print(\"The total difference between the frequency of the 10 most frequent results of count min and the ground truth: \\n%s \\n\"%diff)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Does\tit\tprovide\tbetter\tapproximations\tthan\tthe\tFREQUENT\talgorithm?\tUse\tthe\ttheory\tto\texplain\twhy\t(not)\n",
    "\n",
    "In general, by tuning the parameters we can get similar results with both methods. If we stretch them enough we could also get the actual correct results. However, depending the limitations of time and memory we migth use the one or the other. \n",
    "\n",
    "The FREQUENT algorithm underestimates the frequency while the count min overestimates it.\n",
    "\n",
    "In addition, FREQUENT algorithm seems to use less memory but it focus more in the order of the frequency ( find the x most frequent elements) and not so much on the absolute frequency number. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
