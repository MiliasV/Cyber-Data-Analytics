{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Botnet Classifiaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This function reads the given scenario from the [CTU-13 dataset](http://mcfp.weebly.com/the-ctu-13-dataset-a-labeled-dataset-with-botnet-normal-and-background-traffic.html) and loads it into a Pandas dataframe.\n",
    "It does the following pre-processing steps\n",
    "1. Convert label starting with \"flow=From-Botnet\" to 1 and \"flow=From-Normal\" to 0\n",
    "    - This is done due to the following suggestion in the read-me of the dataset:\n",
    "        - \"Please note that the labels of the flows generated by the malware start with \"From-Botnet\". The labels \"To-Botnet\" are flows sent to the botnet by unknown computers, so they should not be considered malicious perse. Also for the normal computers, the counts are for the labels \"From-Normal\". The labels \"To-Normal\" are flows sent to the botnet by unknown computers, so they should not be considered malicious perse.\"\n",
    "1. Drop rows that contain null values for atleast one of these rows: \"DstAddr\", \"SrcAddr\", \"Dport\", \"Sport\", \"Label\"\n",
    "    -These attributes will be used to build a classifier\n",
    "1. Remove the background flows\n",
    "    - This was suggested by the professor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_file(scenario):\n",
    "    print(\"Reading from file. Scenario: %s\" % scenario)\n",
    "\n",
    "    # Get the path of the file\n",
    "    dir_path = os.path.join(\"..\", \"data\", \"CTU-13-Dataset\", str(scenario))\n",
    "    file_name = filter(lambda x: x.endswith(\".binetflow\"), os.listdir(dir_path))[0]\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "    # Read the csv file in a pandas dataframe\n",
    "    # Convert label: \"flow=From-Botnet\" to 1, label: \"flow=From-Normal\" to 0 and the rest to 2\n",
    "    converters = {\"Label\": lambda x: 1 if x.startswith(\"flow=From-Botnet\") else (0 if x.startswith(\"flow=From-Normal\") else 2)}\n",
    "    df = pd.read_csv(file_path, skip_blank_lines=True, delimiter=\",\", converters=converters)\n",
    "\n",
    "    # Drop rows that contain null values for atleast one of these rows: \"DstAddr\", \"SrcAddr\", \"Dport\", \"Sport\", \"Label\"\n",
    "    df.dropna(subset=[\"DstAddr\", \"SrcAddr\", \"Dport\", \"Sport\", \"Label\"], inplace=True, how=\"any\")\n",
    "\n",
    "    # remove the background flows\n",
    "    df = df[df.Label != 2]\n",
    "\n",
    "    print(\"\\tDone!!\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the scenarios have a big dataset. Library pickle will be used to save the intermediate results to file, to avoid recomputation everytime. Here are some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_pickle_file(file_name):\n",
    "    print(\"Reading pickle file: %s\" % file_name)\n",
    "    path = os.path.join(\"..\", \"data\", \"pickle\", file_name)\n",
    "    pickle_file = open(path, \"r\")\n",
    "    result = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    print(\"\\tDone!!\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_using_pickle(python_object, file_name):\n",
    "    print(\"Writing python object to file: %s\" % file_name)\n",
    "    pickle_file = open(os.path.join(\"..\", \"data\", \"pickle\", file_name), \"w\")\n",
    "    pickle.dump(python_object, pickle_file)\n",
    "    pickle_file.close()\n",
    "    print(\"\\tDone!!\")\n",
    "\n",
    "\n",
    "def save_to_file(content, file_name):\n",
    "    file_ = open(os.path.join(\"..\", \"data\", \"pickle\", file_name), \"w\")\n",
    "    file_.write(content)\n",
    "    file_.close()\n",
    "\n",
    "def read_txt_file(file_name):\n",
    "    file_ = open(os.path.join(\"..\", \"data\", \"pickle\", file_name), \"r\")\n",
    "    data = file_.read()\n",
    "    file_.close()\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINDS feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MINDS](https://www.scopus.com/record/display.uri?eid=2-s2.0-85008699758&origin=inward&txGid=32D807FF8E8677E35E2A45FE0AAED286.wsnAw8kcdt7IPYLO0V48gA%3a2) (MINDS-Minnesota intrusion detectoin system) featureset will be used to build a classifier.\n",
    "\n",
    "The following aggregrate features will be used for each NetFlow.\n",
    "1. The number of NetFlows from the same source IP address as the evaluated NetFlow\n",
    "2. The number of NetFlows toward the same destination host\n",
    "3. The number of NetFlows towards the same destination host from the same source port\n",
    "4. The number of NetFlows from the same source host towards the same destination port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes groupings needed to extract a new feature vector for each of the given scenarios. It saves the groupings to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_groupings_scenarios(scenarios):\n",
    "    # This function groups the dataframe bij the given first_column and save the count of each value\n",
    "    # Furthermore, it groups the Netflows of each of this group by the second_colum and saves the count of each of the value\n",
    "    def nested_grouping(df, first_column, second_column):\n",
    "        print(\"Performing nested grouping with columns: %s, %s\" % (first_column, second_column))\n",
    "        groupby_fc = df.groupby(first_column)\n",
    "        result = {}\n",
    "        fc_values = groupby_fc.groups.keys()\n",
    "        i = 0\n",
    "        for fc_value in fc_values:\n",
    "            # Save the number of occurrences of the current first column value\n",
    "            groupby_fc_current = groupby_fc.get_group(fc_value)\n",
    "            result[fc_value] = (len(groupby_fc_current.index), {})\n",
    "\n",
    "            # Loop over the values of the second column and save their number of occurrences\n",
    "            groupby_fc_sc = groupby_fc_current.groupby(second_column)\n",
    "            for sc_value in groupby_fc_sc.groups.keys():\n",
    "                groupby_fc_sc_current = groupby_fc_sc.get_group(sc_value)\n",
    "                result[fc_value][1][sc_value] = len(groupby_fc_sc_current.index)\n",
    "            i += 1\n",
    "            if i % 200 == 0:\n",
    "                print(\"%s/%s, percentage: %s\" % (i, len(fc_values), float(i) / len(fc_values)))\n",
    "        print(\"\\tDone!!\")\n",
    "        return result\n",
    "    \n",
    "    # This function save the nested grouping to a pickle file\n",
    "    def save_groupings_df(df, first_column, second_column, name):\n",
    "        # Get the groupings\n",
    "        output = nested_grouping(df, first_column, second_column)\n",
    "\n",
    "        # Write the groupings to file\n",
    "        save_using_pickle(output, name)\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        df = read_from_file(scenario)\n",
    "        save_groupings_df(df, \"SrcAddr\", \"Dport\", os.path.join(\"grouping\", \"src_dport_%s.p\" % scenario))\n",
    "        save_groupings_df(df, \"DstAddr\", \"Sport\", os.path.join(\"grouping\", \"dst_sport_%s.p\" % scenario))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads the groupings from file and uses it to extract new MINDS features. At last the feature vector is saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_feature_vectors(scenarios):\n",
    "\n",
    "    def build_feature_vector(scenario):\n",
    "        df = read_from_file(scenario)\n",
    "\n",
    "        src_dport = read_pickle_file(os.path.join(\"grouping\", \"src_dport_%s.p\" % scenario))\n",
    "        dst_sport = read_pickle_file(os.path.join(\"grouping\", \"dst_sport_%s.p\" % scenario))\n",
    "        feature_vector = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            nr_src = src_dport[row[\"SrcAddr\"]][0]\n",
    "            nr_dst = dst_sport[row[\"DstAddr\"]][0]\n",
    "\n",
    "            nr_sport = dst_sport[row[\"DstAddr\"]][1][row[\"Sport\"]]\n",
    "            nr_dport = src_dport[row[\"SrcAddr\"]][1][row[\"Dport\"]]\n",
    "\n",
    "            feature_vector.append([row[\"SrcAddr\"], nr_src, nr_dst, nr_sport, nr_dport, row[\"Label\"]])\n",
    "        return feature_vector\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        feature_vector = build_feature_vector(scenario)\n",
    "        save_using_pickle(feature_vector, os.path.join(\"feature_vector\",\"fv_%s.p\" % scenario))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following set of functions read the computed feature vector of each of the given scenarios. SVM is used for classification.\n",
    "\n",
    "- Evaluation stratey:\n",
    "   - The classifier is evaluated per scenario (suggested in slack) using 3 fold cross validation.\n",
    "   - Packet (NetFlow) level\n",
    "       - Each netflow will be classified as normal or attack\n",
    "   - Host level\n",
    "       - A host is classified as an attacker if it has sent at least one NetFlow which is labeled as attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type can be \"packet_level\" or \"host_level\"\n",
    "def save_classification_error(scenarios, type):\n",
    "\n",
    "    def get_evaluation_metrics(real_labels, predicted_labels, pos_label):\n",
    "        print(\"Calculating evaluation metrics.\")\n",
    "        accuracy = accuracy_score(real_labels, predicted_labels)\n",
    "        f1 = f1_score(real_labels, predicted_labels, pos_label=pos_label)\n",
    "        precision = precision_score(real_labels, predicted_labels,pos_label=pos_label)\n",
    "        recall = recall_score(real_labels, predicted_labels, pos_label=pos_label)\n",
    "        confusion_matrix_ = confusion_matrix(real_labels, predicted_labels)\n",
    "\n",
    "        resulting_metrics = {\"accuracy\": accuracy, \"f1\": f1,\n",
    "                             \"precision\":precision, \"recall\": recall, \"confusion_matrix\": confusion_matrix_}\n",
    "        print(\"\\tDone!!\")\n",
    "        return resulting_metrics\n",
    "\n",
    "    def evaluate_host_level(real_labels, predicted_labels, host_ips):\n",
    "        index_host_ips = {}\n",
    "        for index in range(len(host_ips)):\n",
    "            if host_ips[index] not in index_host_ips.keys():\n",
    "                index_host_ips[host_ips[index]] = []\n",
    "            index_host_ips[host_ips[index]].append(index)\n",
    "\n",
    "        new_real_labels = []\n",
    "        new_predicted_labels = []\n",
    "        for host_ip in host_ips:\n",
    "            new_real_labels.append(\"1\" if \"1\" in real_labels[index_host_ips[host_ip]] else \"0\")\n",
    "            new_predicted_labels.append(\"1\" if \"1\" in predicted_labels[index_host_ips[host_ip]] else \"0\")\n",
    "\n",
    "        return new_real_labels, new_predicted_labels\n",
    "\n",
    "    def evaluate(scenario):\n",
    "        # Read the feature vector for the given scenario\n",
    "        feature_vector = np.array(read_pickle_file(os.path.join(\"feature_vector\", \"fv_%s.p\" % scenario)))\n",
    "\n",
    "        # Get the training set and the labels\n",
    "        X = feature_vector[:, 1:5]\n",
    "        y = feature_vector[:, 5:6].flatten()\n",
    "        host_ips = feature_vector[:, 0:1].flatten()\n",
    "\n",
    "        # Perform three fold cross validation\n",
    "        real_labels = []\n",
    "        predicted_labels = []\n",
    "        test_host_ips = []\n",
    "        k_fold = StratifiedKFold(n_splits=3)\n",
    "        for train, test in k_fold.split(X, y):\n",
    "\n",
    "            # Build a svm classifier\n",
    "            classifier = SVC()\n",
    "            classifier.fit(X[train], y[train])\n",
    "\n",
    "            real_labels.extend(y[test])\n",
    "            predicted_labels.extend(classifier.predict(X[test]))\n",
    "            test_host_ips.extend(host_ips[test])\n",
    "\n",
    "        pos_label = \"1\"\n",
    "        if type == \"host_level\":\n",
    "            real_labels, predicted_labels = evaluate_host_level(np.array(real_labels), np.array(predicted_labels), test_host_ips)\n",
    "\n",
    "        return get_evaluation_metrics(real_labels, predicted_labels, pos_label)\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        evaluation_metrics = evaluate(scenario)\n",
    "        evaluation_metrics_str = reduce(lambda x, y: \"%s\\n%s\" % (x,y), [\"%s = %s\" % (metric, evaluation_metrics[metric]) for metric in evaluation_metrics.keys()])\n",
    "        save_to_file(evaluation_metrics_str, os.path.join(\"evaluation_metric\", type, \"result_%s.txt\" % scenario))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scenarios have been computed off-line. We let our computer running the whole night. The results were saved in disk.\n",
    "Scenario 3 and 10 could not be calculated due to its huge size and memory limitation of my laptop. Python outputted memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the different types (attack and normal) differs per scenario. To cope with this multiple evaluation metrics are considered. The F1 score, precision and accuracy are caculated in reference to the attack class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_evaluation_metrics(scenario):\n",
    "    result = \"Packet level:\\n\"\n",
    "    result += read_txt_file(os.path.join(\"evaluation_metric\", \"packet_level\", \"result_%s.txt\" % scenario))\n",
    "    result += \"\\n\\nHost level:\\n\"\n",
    "    result += read_txt_file(os.path.join(\"evaluation_metric\", \"host_level\", \"result_%s.txt\" % scenario ))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet level:\n",
      "f1 = 0.989850414441\n",
      "recall = 1.0\n",
      "confusion_matrix = [[29405   840]\n",
      " [    0 40961]]\n",
      "precision = 0.979904786967\n",
      "accuracy = 0.9882032413\n",
      "\n",
      "Host level:\n",
      "f1 = 0.732054295084\n",
      "recall = 1.0\n",
      "confusion_matrix = [[  260 29985]\n",
      " [    0 40961]]\n",
      "precision = 0.57735460773\n",
      "accuracy = 0.578897845687\n"
     ]
    }
   ],
   "source": [
    "print(read_evaluation_metrics(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet level:\n",
      "f1 = 0.994774595031\n",
      "recall = 1.0\n",
      "confusion_matrix = [[ 8853   220]\n",
      " [    0 20941]]\n",
      "precision = 0.989603515902\n",
      "accuracy = 0.992670087293\n",
      "\n",
      "Host level:\n",
      "f1 = 0.82336288753\n",
      "recall = 1.0\n",
      "confusion_matrix = [[   88  8985]\n",
      " [    0 20941]]\n",
      "precision = 0.699759406536\n",
      "accuracy = 0.700639701473\n"
     ]
    }
   ],
   "source": [
    "print(read_evaluation_metrics(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet level:\n",
      "f1 = 0.912087912088\n",
      "recall = 0.838383838384\n",
      "confusion_matrix = [[25184     0]\n",
      " [  240  1245]]\n",
      "precision = 1.0\n",
      "accuracy = 0.991000787431\n",
      "\n",
      "Host level:\n",
      "f1 = 1.0\n",
      "recall = 1.0\n",
      "confusion_matrix = [[25184     0]\n",
      " [    0  1485]]\n",
      "precision = 1.0\n",
      "accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(read_evaluation_metrics(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet level:\n",
      "f1 = 0.975554292211\n",
      "recall = 0.952275249723\n",
      "confusion_matrix = [[4657    0]\n",
      " [  43  858]]\n",
      "precision = 1.0\n",
      "accuracy = 0.992263404102\n",
      "\n",
      "Host level:\n",
      "f1 = 1.0\n",
      "recall = 1.0\n",
      "confusion_matrix = [[4657    0]\n",
      " [   0  901]]\n",
      "precision = 1.0\n",
      "accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(read_evaluation_metrics(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the evaluation metrics of the other scenarios using print(read_evaluation_metrics(scenario))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that the perfomance of the classifier varies a lot between different scenarios. \n",
    "This means that the behavior of some some botnets are eaiser to model than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
